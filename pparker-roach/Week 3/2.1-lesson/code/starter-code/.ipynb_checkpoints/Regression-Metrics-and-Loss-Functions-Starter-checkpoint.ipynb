{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Metrics and Loss Functions\n",
    "\n",
    "We've seen two examples of _loss functions_ earlier in the week in the context of regularization.\n",
    "\n",
    "For a model of the form $y = f(x) + \\epsilon$ with predictions $\\hat{y}_i$ and true values $y_i$, we have:\n",
    "\n",
    "* The sum of squared errors:\n",
    "$$\\text{SSE} = \\sum_{i}{\\left(\\hat{y}_i - y_i \\right)^2}$$\n",
    "* A Regularlized version:\n",
    "If our model parameters are $\\theta_i$ and our regularization parameter is $\\alpha$, then the loss function took the form:\n",
    "$$\\text{L} = \\sum_{i}{\\left(\\hat{y}_i - y_i \\right)^2 + \\alpha \\theta_i}$$\n",
    "\n",
    "In this lesson we're going to dig deeper into loss functions and their applications. Different loss functions are useful in different scenarios and there are two very popular loss functions that are used in conjuction with regression. In this case they are sometimes referred to as _regression metrics_.\n",
    "\n",
    "The first is the _root mean squared error_ or _RMSE_ and it is the mean of the squared errors. If we have $n$ regression points and their predictions, the [RMSE](https://en.wikipedia.org/wiki/Root-mean-square_deviation) is:\n",
    "\n",
    "$$\\text{RMSE} = \\sqrt{\\frac{\\sum_{i}{\\left(\\hat{y}_i - y_i \\right)^2}}{n}}$$\n",
    "\n",
    "The second is the _mean absolute error_ or _MAE_, and it differs by use of an absolute value instead of a square. The [MAE](https://en.wikipedia.org/wiki/Average_absolute_deviation) is:\n",
    "\n",
    "$$\\text{MAE} = \\frac{\\sum_{i}{|\\hat{y}_i - y_i |}}{n}$$\n",
    "\n",
    "## Why have different regression metrics?\n",
    "\n",
    "You might be thinking, _what's all the fuss about_? It turns out that there are lots of good reasons to use different loss functions. We've seen one -- regularization -- and now we'll consider the effects of outliers on these two metrics.\n",
    "\n",
    "First let's try a very simplified statistics problem. Given a dataset, how can we summarize it with a single number? Do you know any ways?\n",
    "\n",
    "This is equivalent to fitting a constant model to the data. It turns out that the _mean_ minimizes the RMSE and the _median_ minimizes the MAE. By analogy, when fitting a model, MAE is more tolerant to outliers. In other words, the degree of error of an outlier has a large impact when using RMSE versus the MAE. Since the choice of loss function affects model fit, it's important to consider how you want errors to impact your models.\n",
    "\n",
    "**Summary**\n",
    "* Use MAE when how far off an error is makes little difference\n",
    "* Use RMSE when more extreme errors should have a large impact\n",
    "\n",
    "Finally, note that linear regressions with MAE instead of RMSE are called _least absolute deviation_ regressions rather than least squares regressions.\n",
    "\n",
    "### Bonus: Modes\n",
    "\n",
    "It turns out the _mode_ minimizes the sum:\n",
    "$$\\frac{\\sum_{i}{|\\hat{y}_i - y_i |^{0}}}{n}$$\n",
    "where $0^0=0$ and $x^0=1$ otherwise. Can you see why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guided practice\n",
    "\n",
    "Let's compute the RMSE and the MAE for a sample data set. Let's say we had a quadratic function that we fit a line to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 2, 5, 10]\n",
      "[-2, 0, 2, 4, 6]\n"
     ]
    }
   ],
   "source": [
    "xs = [-1, 0, 1, 2, 3]\n",
    "ys = [x*x + 1 for x in xs] # true values\n",
    "predictions = [2*x for x in xs]\n",
    "print ys\n",
    "print predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First do the calculation by hand to see how large each term is\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 2.60768096208\n",
      "MAE: 2.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import math\n",
    "print \"RMSE:\", math.sqrt(mean_squared_error(ys, predictions))\n",
    "print \"MAE:\", mean_absolute_error(ys, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add an outlier to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 5.81664278887\n",
      "MAE: 3.83333333333\n"
     ]
    }
   ],
   "source": [
    "xs.append(4)\n",
    "ys.append(17)\n",
    "predictions.append(30)\n",
    "\n",
    "print \"RMSE:\", math.sqrt(mean_squared_error(ys, predictions))\n",
    "print \"MAE:\", mean_absolute_error(ys, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the impact of adding outliers to our data. The effect on the RMSE was large, a factor of 2.23, versus the impact on the MAE with a factor of 1.92. This behavior is expected because RMSE gives more weight to outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indepedent Practice\n",
    "\n",
    "Let's explore two scenarios to obtain a better understanding of RMSE and MAE. First let's fit two models to the same set of data, the data above. To do the least mean absolute error we will use `statsmodels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "# Make the plots bigger\n",
    "plt.rcParams['figure.figsize'] = 10, 10\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's add a few more points\n",
    "xs.append(2.5)\n",
    "ys.append(17)\n",
    "\n",
    "xs.append(1.5)\n",
    "ys.append(-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>QuantReg Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>y</td>        <th>  Pseudo R-squared:  </th> <td>  0.3600</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>             <td>QuantReg</td>     <th>  Bandwidth:         </th> <td>   19.62</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>          <td>Least Squares</td>  <th>  Sparsity:          </th> <td>   28.42</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Thu, 10 Nov 2016</td> <th>  No. Observations:  </th>  <td>     8</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>11:20:15</td>     <th>  Df Residuals:      </th>  <td>     6</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  Df Model:          </th>  <td>     1</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    1.0000</td> <td>    7.366</td> <td>    0.136</td> <td> 0.896</td> <td>  -17.023</td> <td>   19.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x</th>         <td>    3.0000</td> <td>    3.315</td> <td>    0.905</td> <td> 0.400</td> <td>   -5.111</td> <td>   11.111</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                         QuantReg Regression Results                          \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   Pseudo R-squared:               0.3600\n",
       "Model:                       QuantReg   Bandwidth:                       19.62\n",
       "Method:                 Least Squares   Sparsity:                        28.42\n",
       "Date:                Thu, 10 Nov 2016   No. Observations:                    8\n",
       "Time:                        11:20:15   Df Residuals:                        6\n",
       "                                        Df Model:                            1\n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      1.0000      7.366      0.136      0.896     -17.023      19.023\n",
       "x              3.0000      3.315      0.905      0.400      -5.111      11.111\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(np.array([xs, ys]).transpose(), columns=[\"x\", \"y\"])\n",
    "df.columns = [\"x\", \"y\"]\n",
    "mod = smf.quantreg('y ~ x', df)\n",
    "res = mod.fit(q=.5)\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generated a fit of $y = 3 x + 1$. Let's see what a linear regression yields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandertam/anaconda/lib/python2.7/site-packages/scipy/stats/stats.py:1326: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=8\n",
      "  \"anyway, n=%i\" % int(n))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.477</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.390</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   5.473</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 10 Nov 2016</td> <th>  Prob (F-statistic):</th>  <td>0.0579</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:20:15</td>     <th>  Log-Likelihood:    </th> <td> -24.966</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>     8</td>      <th>  AIC:               </th> <td>   53.93</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>     6</td>      <th>  BIC:               </th> <td>   54.09</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    0.3844</td> <td>    3.282</td> <td>    0.117</td> <td> 0.911</td> <td>   -7.647</td> <td>    8.416</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    3.4558</td> <td>    1.477</td> <td>    2.340</td> <td> 0.058</td> <td>   -0.159</td> <td>    7.070</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 2.698</td> <th>  Durbin-Watson:     </th> <td>   1.870</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.260</td> <th>  Jarque-Bera (JB):  </th> <td>   0.637</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.688</td> <th>  Prob(JB):          </th> <td>   0.727</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.141</td> <th>  Cond. No.          </th> <td>    3.64</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.477\n",
       "Model:                            OLS   Adj. R-squared:                  0.390\n",
       "Method:                 Least Squares   F-statistic:                     5.473\n",
       "Date:                Thu, 10 Nov 2016   Prob (F-statistic):             0.0579\n",
       "Time:                        11:20:15   Log-Likelihood:                -24.966\n",
       "No. Observations:                   8   AIC:                             53.93\n",
       "Df Residuals:                       6   BIC:                             54.09\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.3844      3.282      0.117      0.911      -7.647       8.416\n",
       "x1             3.4558      1.477      2.340      0.058      -0.159       7.070\n",
       "==============================================================================\n",
       "Omnibus:                        2.698   Durbin-Watson:                   1.870\n",
       "Prob(Omnibus):                  0.260   Jarque-Bera (JB):                0.637\n",
       "Skew:                          -0.688   Prob(JB):                        0.727\n",
       "Kurtosis:                       3.141   Cond. No.                         3.64\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "X = np.array(xs).transpose()\n",
    "X = sm.add_constant(X)\n",
    "# Fit and summarize OLS model\n",
    "mod = sm.OLS(ys, X)\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This yielded a fit of $y = 3.4558 x + 0.3844$.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Plot the data with both functions. Which do you think fits the data better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1 = lambda x: 3*x + 1\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's explore another scenario. Linear regression has five major assumptions, one of which is called _constant variance_ or _homoscedasticity_. It means that the errors are distributed with the same variance about the best fit line regardless of the value of the independent variables.\n",
    "\n",
    "For example, a persistant level of background noise can cause regression metrics to be poorly estimated. Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAAFVCAYAAAA+OJwpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAIZAAADEMgAABiAQAYAwAAEMgAABvh/oJCT23Y5g0wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117f8fbd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "from scipy.stats import norm\n",
    "# Generate some data\n",
    "xs = list(np.arange(0, 10, 0.1))\n",
    "ys = [2*x + norm.pdf(0, 1) for x in xs]\n",
    "# Add random background noise\n",
    "xs2 = [10 * random.random() for i in range(20)]\n",
    "ys2 = [20 * random.random() for i in range(20)]\n",
    "\n",
    "# Plot the data sets\n",
    "plt.scatter(xs, ys, color='b')\n",
    "plt.scatter(xs2, ys2, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine the data\n",
    "xs.extend(xs2)\n",
    "ys.extend(ys2)\n",
    "df = pd.DataFrame(np.array([xs, ys]).transpose(), columns=['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fit a line to the data\n",
    "# Compute the RMSE and the MAE\n",
    "# Plot the regression line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now try a MAE regression with statsmodels and plot it.\n",
    "# You should see a much better fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the data and the two fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
