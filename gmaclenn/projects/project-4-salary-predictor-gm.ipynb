{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model, metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import *\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a new function that allows for a range with floats\n",
    "def xfrange(start, stop, step):\n",
    "    i = 0\n",
    "    while start + i * step < stop:\n",
    "        yield start + i * step\n",
    "        i += 1\n",
    "        \n",
    "# function to look at the coefficients of the given model and df\n",
    "def examine_coefficients(model, df):\n",
    "    return pd.DataFrame(\n",
    "        { 'Coefficient' : model.coef_[0] , 'Feature' : df.columns}\n",
    "    ).sort_values(by='Coefficient')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import DataFrames and Clean Data\n",
    "### Describe the data:\n",
    "* id_and_python.csv - 309 rows, details if python was found in the job description, pulled from indeed.com\n",
    "* id_salary.csv - 1359 rows, details if the salary listed was over 90k, pulled from indeed.com\n",
    "* id_years.csv - 999 rows, details if the # of years required was was listed job description, values of 0, 1, 2, 3, and 4+ years, pulled from indeed.com\n",
    "* phd.csv - 719 rows, details if phd was found in the job description, pulled from indeed.com\n",
    "* startup_df.csv - 2489 rows, details if startup was found in the job description, pulled from indeed.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start 305 end 305 : 0 entries removed\n",
      "start 1125 end 1125 : 0 entries removed\n",
      "start 1000 end 768 : 232 entries removed\n",
      "start 720 end 573 : 147 entries removed\n",
      "start 2490 end 364 : 2126 entries removed\n",
      "\n",
      "\n",
      "                    id  over_90k  python  ys_0  ys_1  ys_2  ys_3  ys_4  phd  \\\n",
      "0  jl_85c4e91c561780aa         0     1.0   0.0   1.0   0.0   0.0   0.0  0.0   \n",
      "1  jl_e632343c455d80f9         0     1.0   1.0   0.0   0.0   0.0   0.0  0.0   \n",
      "2  jl_1b5a168dfc7b2712         0     0.0   0.0   0.0   0.0   1.0   0.0  0.0   \n",
      "3  jl_05654b2739edb3dc         0     0.0   1.0   0.0   0.0   0.0   0.0  0.0   \n",
      "4  jl_1a7766c45b1abbeb         0     0.0   0.0   0.0   0.0   1.0   0.0  1.0   \n",
      "\n",
      "   startup  \n",
      "0      0.0  \n",
      "1      0.0  \n",
      "2      0.0  \n",
      "3      1.0  \n",
      "4      0.0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in the 5 separate dataframes pulled from our indeed.com web scraping\n",
    "id_python = pd.read_csv('../../DSI-BOS-students/timote_hogan/Project-04/id_and_Python.csv')\n",
    "id_salary = pd.read_csv('../../DSI-BOS-students/timote_hogan/Project-04/id_salary.csv')\n",
    "id_years = pd.read_csv('../../DSI-BOS-students/timote_hogan/Project-04/id_years.csv')\n",
    "id_phd = pd.read_csv('../../DSI-BOS-students/timote_hogan/Project-04/phd_df.csv')\n",
    "id_startup = pd.read_csv('../../DSI-BOS-students/timote_hogan/Project-04/startup_df.csv')\n",
    "\n",
    "id_phd=id_phd.rename(columns = {'job_id':'id'}) # renames the job_id column to id\n",
    "\n",
    "# run through the dataframes and list how many rows we removed\n",
    "df_list = [id_python,id_salary,id_years, id_phd, id_startup]\n",
    "for i,df in enumerate(df_list):\n",
    "    start_length = len(df)\n",
    "    df = df.drop_duplicates(subset = 'id')\n",
    "    end_length = len(df)\n",
    "    print 'start', start_length, 'end', end_length, ':', start_length - end_length, \"entries removed\"\n",
    "\n",
    "print '\\n'\n",
    "\n",
    "# remove duplicate id's\n",
    "id_phd = id_phd.drop_duplicates(subset='id')\n",
    "id_salary = id_salary.drop_duplicates(subset='id')\n",
    "id_phd = id_phd.drop_duplicates(subset='id')\n",
    "id_startup = id_startup.drop_duplicates(subset='id')\n",
    "id_years = id_years.drop_duplicates(subset='id')\n",
    "\n",
    "# split out the years column to dummy variables\n",
    "id_years_dummies = pd.get_dummies(id_years.years,prefix='ys')\n",
    "id_years = pd.concat([id_years, id_years_dummies],axis=1) # combine the two dataframes back together\n",
    "# print id_years.head()\n",
    "\n",
    "# Using left merges on the id_salary df so we keep the data with salary info\n",
    "salary_df = pd.merge(id_salary, id_python, how='left', on='id')\n",
    "salary_df = pd.merge(salary_df, id_years, how='left', on='id')\n",
    "salary_df = pd.merge(salary_df, id_phd, how='left', on='id')\n",
    "salary_df = pd.merge(salary_df, id_startup, how='left', on='id')\n",
    "\n",
    "# drop the following lists from the dataframe that are not needed\n",
    "drop_list = [u'Unnamed: 0_x', u'Unnamed: 0_y', u'Unnamed: 0_x', u'years', u'Unnamed: 0_y', u'Unnamed: 0', u'title']\n",
    "salary_df = salary_df.drop(drop_list, axis=1)\n",
    "\n",
    "# fill the NaN values with 0. Since all the Dataframes we merged were 1 or nothing we can safely fill these values\n",
    "salary_df = salary_df.fillna(0)\n",
    "\n",
    "# shortens the columns and lowercases\n",
    "salary_df = salary_df.rename(columns= {'Python':'python', 'has_phd':'phd','has_startup':'startup'})\n",
    "\n",
    "print salary_df.head(), '\\n'\n",
    "\n",
    "# export to CSV for data analysis\n",
    "salary_df.to_csv('/Users/gmaclenn/DSI-BOS-students/gmaclenn/source_files/salary_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(731, 8) (731,)\n",
      "(394, 8) (394,)\n"
     ]
    }
   ],
   "source": [
    "predictor_cols = ['python', 'phd', 'startup', 'ys_0', 'ys_1', 'ys_2', 'ys_3', 'ys_4']\n",
    "target_cols = ['over_90k'] \n",
    "\n",
    "X = salary_df[predictor_cols]\n",
    "y = np.ravel(salary_df[target_cols]) # need a 1-d array (use ravel)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35)\n",
    "\n",
    "print X_train.shape, y_train.shape # confirm the train test split gives us the right dimensions\n",
    "print X_test.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Predicted Over 90k  Predicted Under 90k\n",
      "Over 90k                  112                   98\n",
      "Under 90k                  59                  125\n"
     ]
    }
   ],
   "source": [
    "logit = linear_model.LogisticRegression()\n",
    "job_model = logit.fit(X_train, y_train)\n",
    "predictions = logit.predict(X_test)\n",
    "\n",
    "conmat = np.array(confusion_matrix(y_test, predictions, labels=[1,0]))\n",
    "\n",
    "confusion_df = pd.DataFrame(conmat, index=['Over 90k', 'Under 90k'], columns=['Predicted Over 90k', 'Predicted Under 90k'])\n",
    "\n",
    "print confusion_df\n",
    "# print '\\n', classification_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine Model Performance and Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: \t0.60152284264\n",
      "Precision: \t0.654970760234\n",
      "Recall: \t0.533333333333 \n",
      "\n",
      "   Coefficient  Feature\n",
      "4    -1.077536     ys_1\n",
      "3    -0.345652     ys_0\n",
      "5    -0.197909     ys_2\n",
      "6     0.200939     ys_3\n",
      "1     0.683471      phd\n",
      "7     0.858624     ys_4\n",
      "0     1.170492   python\n",
      "2     1.571412  startup\n"
     ]
    }
   ],
   "source": [
    "TP = float(confusion_df.iloc[0][0])\n",
    "TN = float(confusion_df.iloc[1][1])\n",
    "FP = float(confusion_df.iloc[1][0])\n",
    "FN = float(confusion_df.iloc[0][1])\n",
    "\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "Precision = TP / (TP + FP)\n",
    "Recall = TP / (TP + FN)\n",
    "\n",
    "print \"Accuracy: \\t\", Accuracy\n",
    "print \"Precision: \\t\", Precision\n",
    "print \"Recall: \\t\", Recall, '\\n'\n",
    "\n",
    "print examine_coefficients(job_model, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Grid Search to optimize for f1_score\n",
    "f1_score is essentially the weighted average of precision and recall. Optimizing for these two metrics will give us a good balanced model without skewing one or the other too heavily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    0.2s\n",
      "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'penalty': ['l1', 'l2'], 'C': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='f1_macro', verbose=1)"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_vals = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "penalties = ['l1','l2']\n",
    "\n",
    "gs_1 = GridSearchCV(logit, {'penalty':penalties, 'C':C_vals}, cv=5, scoring='f1_macro', verbose=1)\n",
    "gs_1.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'penalty': 'l2', 'C': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print gs_1.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the default settings for the Logistic Regression model are C=1.0 and penalty = l2, we do not need to modify the previous model at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify the thresholds to play with accuracy and recall\n",
    "The function below iterates over the range 0.5 (standard cutoff value) to 0.75 and creates a dictionary of the probability cutoff as a key and the accuracy, precision and recall of the model as values. We then return the max Precision value and it's associated cutoff value to demonstrate a cutoff value that would maximize precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Probability Cutoff: \t0.7\n",
      "Maximized Precision:\t\t0.782608695652\n",
      "Accuracy: \t\t\t0.565989847716\n",
      "Recall: \t\t\t0.257142857143\n"
     ]
    }
   ],
   "source": [
    "prob_df = pd.DataFrame(logit.predict_proba(X_test), columns = ['prob0', 'prob1'])\n",
    "\n",
    "# Modify the threshold to maximize Precision\n",
    "\n",
    "# create a dictionary\n",
    "max_precision_dict = {}\n",
    "\n",
    "for prob_cutoff in xfrange(0.5,0.75,0.01):\n",
    "    prob_df['higher_threshold'] = [1 if x >= prob_cutoff else 0 for x in prob_df.prob1.values]\n",
    "\n",
    "    # Create the confusion matrix\n",
    "    conmat2 = np.array(confusion_matrix(y_test, prob_df.higher_threshold.values, labels=[1,0]))\n",
    "\n",
    "    confusion_df2 = pd.DataFrame(conmat2, index=['Over 90k', 'Under 90k'], columns=['Predicted Over 90k', 'Predicted Under 90k'])\n",
    "\n",
    "    # assign variables for measuring statistics\n",
    "    TP = float(confusion_df2.iloc[0][0])\n",
    "    TN = float(confusion_df2.iloc[1][1])\n",
    "    FP = float(confusion_df2.iloc[1][0])\n",
    "    FN = float(confusion_df2.iloc[0][1])\n",
    "\n",
    "    Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    Precision = TP / (TP + FP)\n",
    "    Recall = TP / (TP + FN)\n",
    "    \n",
    "    # creates dictionary with the probability cutoff as the key and\n",
    "    # the metrics precision, accuracy & recall as values\n",
    "    max_precision_dict[prob_cutoff] = [Precision, Accuracy, Recall]\n",
    "    \n",
    "# finds the maximum value from the newly created dictionary\n",
    "import operator\n",
    "max_precision = max(max_precision_dict.iteritems(), key=operator.itemgetter(1))\n",
    "\n",
    "print \"Optimal Probability Cutoff: \\t\", max_precision[0]\n",
    "print \"Maximized Precision:\\t\\t\", max_precision[1][0]\n",
    "print \"Accuracy: \\t\\t\\t\", max_precision[1][1]\n",
    "print \"Recall: \\t\\t\\t\", max_precision[1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Predicted Over 90k  Predicted Under 90k\n",
      "Over 90k                   54                  156\n",
      "Under 90k                  15                  169 \n",
      "\n",
      "Original Accuracy: \t0.60152284264\n",
      "Original Precision: \t0.654970760234\n",
      "Original Recall: \t0.533333333333 \n",
      "\n",
      "Accuracy:  \t\t0.565989847716\n",
      "Precision:  \t\t0.782608695652\n",
      "Recall:  \t\t0.257142857143\n"
     ]
    }
   ],
   "source": [
    "prob_df = pd.DataFrame(logit.predict_proba(X_test), columns = ['prob0', 'prob1'])\n",
    "\n",
    "# print prob_df\n",
    "prob_df['higher_threshold'] = [1 if x >= 0.7 else 0 for x in prob_df.prob1.values]\n",
    "# print prob_df.head(), '\\n'\n",
    "\n",
    "# Create the confusion matrix\n",
    "conmat3 = np.array(confusion_matrix(y_test, prob_df.higher_threshold.values, labels=[1,0]))\n",
    "\n",
    "confusion_df3 = pd.DataFrame(conmat3, index=['Over 90k', 'Under 90k'], columns=['Predicted Over 90k', 'Predicted Under 90k'])\n",
    "print confusion_df3, '\\n'\n",
    "\n",
    "# assign variables for measuring statistics\n",
    "TP = float(confusion_df2.iloc[0][0])\n",
    "TN = float(confusion_df2.iloc[1][1])\n",
    "FP = float(confusion_df2.iloc[1][0])\n",
    "FN = float(confusion_df2.iloc[0][1])\n",
    "\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "Precision = TP / (TP + FP)\n",
    "Recall = TP / (TP + FN)\n",
    "\n",
    "print \"Original Accuracy: \\t\", metrics.accuracy_score(y_test, predictions)\n",
    "print \"Original Precision: \\t\", metrics.precision_score(y_test, predictions)\n",
    "print \"Original Recall: \\t\", metrics.recall_score(y_test, predictions), '\\n'\n",
    "\n",
    "print \"Accuracy: \", '\\t\\t', Accuracy,\n",
    "print \"Precision: \", '\\t\\t', Precision\n",
    "print \"Recall: \", '\\t\\t', Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can see that modifying the probability cutoff range allows us to play with the Precision & Recall. In an instance where we wanted to minimize the instance of false positives as much as possible, to increase Precision, the Recall value takes a big hit. This is an instance of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Gridsearch to optimize the precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks       | elapsed:    0.2s\n",
      "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'penalty': ['l1', 'l2'], 'C': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='precision', verbose=1)"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_vals = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "penalties = ['l1','l2']\n",
    "\n",
    "gs_2 = GridSearchCV(logit, {'penalty':penalties, 'C':C_vals}, cv=5, scoring='precision', verbose=1)\n",
    "gs_2.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'penalty': 'l1', 'C': 10.0}\n"
     ]
    }
   ],
   "source": [
    "print gs_2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.69      0.62       184\n",
      "          1       0.66      0.54      0.59       210\n",
      "\n",
      "avg / total       0.62      0.61      0.61       394\n",
      "\n",
      "           Predicted Over 90k  Predicted Under 90k\n",
      "Over 90k                  113                   97\n",
      "Under 90k                  57                  127 \n",
      "\n",
      "Original Accuracy: \t0.60152284264\n",
      "Original Precision: \t0.654970760234\n",
      "Original Recall: \t0.533333333333 \n",
      "\n",
      "Accuracy: \t0.609137055838\n",
      "Precision: \t0.664705882353\n",
      "Recall: \t0.538095238095 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "logit_final = linear_model.LogisticRegression(C=10, penalty='l1')\n",
    "logit_final.fit(X,y)\n",
    "\n",
    "y_pred_final = logit_final.predict(X_test)\n",
    "print classification_report(y_test, y_pred_final)\n",
    "\n",
    "conmat4 = np.array(confusion_matrix(y_test, y_pred_final, labels=[1,0]))\n",
    "\n",
    "confusion_df4 = pd.DataFrame(conmat4, index=['Over 90k', 'Under 90k'], columns=['Predicted Over 90k', 'Predicted Under 90k'])\n",
    "print confusion_df4, '\\n'\n",
    "\n",
    "\n",
    "print \"Original Accuracy: \\t\", metrics.accuracy_score(y_test, predictions)\n",
    "print \"Original Precision: \\t\", metrics.precision_score(y_test, predictions)\n",
    "print \"Original Recall: \\t\", metrics.recall_score(y_test, predictions), '\\n'\n",
    "\n",
    "print \"Accuracy: \\t\", metrics.accuracy_score(y_test, y_pred_final)\n",
    "print \"Precision: \\t\", metrics.precision_score(y_test, y_pred_final)\n",
    "print \"Recall: \\t\", metrics.recall_score(y_test, y_pred_final), '\\n'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the modifications we made to the penalty and C values. The model hardly changed at all with the updated parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
