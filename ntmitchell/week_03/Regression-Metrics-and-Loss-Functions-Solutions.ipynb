{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Metrics and Loss Functions\n",
    "\n",
    "We've seen two examples of _loss functions_ earlier in the week in the context of regularization.\n",
    "\n",
    "For a model of the form $y = f(x) + \\epsilon$ with predictions $\\hat{y}_i$ and true values $y_i$, we have:\n",
    "\n",
    "* The sum of squared errors:\n",
    "$$\\text{SSE} = \\sum_{i}{\\left(\\hat{y}_i - y_i \\right)^2}$$\n",
    "* A Regularlized version:\n",
    "If our model parameters are $\\theta_i$ and our regularization parameter is $\\alpha$, then the loss function took the form:\n",
    "$$\\text{L} = \\sum_{i}{\\left(\\hat{y}_i - y_i \\right)^2 + \\alpha \\theta_i}$$\n",
    "\n",
    "In this lesson we're going to dig deeper into loss functions and their applications. Different loss functions are useful in different scenarios and there are two very popular loss functions that are used in conjuction with regression. In this case they are sometimes referred to as _regression metrics_.\n",
    "\n",
    "The first is the _root mean squared error_ or _RMSE_ and it is the mean of the squared errors. If we have $n$ regression points and their predictions, the [RMSE](https://en.wikipedia.org/wiki/Root-mean-square_deviation) is:\n",
    "\n",
    "$$\\text{RMSE} = \\sqrt{\\frac{\\sum_{i}{\\left(\\hat{y}_i - y_i \\right)^2}}{n}}$$\n",
    "\n",
    "The second is the _mean absolute error_ or _MAE_, and it differs by use of an absolute value instead of a square. The [MAE](https://en.wikipedia.org/wiki/Average_absolute_deviation) is:\n",
    "\n",
    "$$\\text{MAE} = \\frac{\\sum_{i}{|\\hat{y}_i - y_i |}}{n}$$\n",
    "\n",
    "## Why have different regression metrics?\n",
    "\n",
    "You might be thinking, _what's all the fuss about_? It turns out that there are lots of good reasons to use different loss functions. We've seen one -- regularization -- and now we'll consider the effects of outliers on these two metrics.\n",
    "\n",
    "First let's try a very simplified statistics problem. Given a dataset, how can we summarize it with a single number? Do you know any ways?\n",
    "\n",
    "This is equivalent to fitting a constant model to the data. It turns out that the _mean_ minimizes the RMSE and the _median_ minimizes the MAE. By analogy, when fitting a model, MAE is more tolerant to outliers. In other words, the degree of error of an outlier has a large impact when using RMSE versus the MAE. Since the choice of loss function affects model fit, it's important to consider how you want errors to impact your models.\n",
    "\n",
    "**Summary**\n",
    "* Use MAE when how far off an error is makes little difference\n",
    "* Use RMSE when more extreme errors should have a large impact\n",
    "\n",
    "Finally, note that linear regressions with MAE instead of RMSE are called _least absolute deviation_ regressions rather than least squares regressions.\n",
    "\n",
    "### Bonus: Modes\n",
    "\n",
    "It turns out the _mode_ minimizes the sum:\n",
    "$$\\frac{\\sum_{i}{|\\hat{y}_i - y_i |^{0}}}{n}$$\n",
    "where $0^0=0$ and $x^0=1$ otherwise. Can you see why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guided practice\n",
    "\n",
    "Let's compute the RMSE and the MAE for a sample data set. Let's say we had a quadratic function that we fit a line to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 2, 5, 10]\n",
      "[-2, 0, 2, 4, 6]\n"
     ]
    }
   ],
   "source": [
    "xs = [-1, 0, 1, 2, 3]\n",
    "ys = [x*x + 1 for x in xs] # true values\n",
    "predictions = [2*x for x in xs]\n",
    "print (ys)\n",
    "print (predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First do the calculation by hand to see how large each term is\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 6.8\n",
      "MAE: 2.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "print (\"RMSE:\", mean_squared_error(ys, predictions))\n",
    "print (\"MAE:\", mean_absolute_error(ys, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add an outlier to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 33.8333333333\n",
      "MAE: 3.83333333333\n"
     ]
    }
   ],
   "source": [
    "xs.append(4)\n",
    "ys.append(17)\n",
    "predictions.append(30)\n",
    "\n",
    "print (\"RMSE:\", mean_squared_error(ys, predictions))\n",
    "print (\"MAE:\", mean_absolute_error(ys, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the impact on the RMSE was large, a factor of 8, versus the impact on the MAE with a factor of 2.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indepedent Practice\n",
    "\n",
    "Let's explore two scenarios to obtain a better understanding of RMSE and MAE. First let's fit two models to the same set of data, the data above. To do the least mean absolute error we will use `statsmodels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "# Make the plots bigger\n",
    "plt.rcParams['figure.figsize'] = 10, 10\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's add a few more points\n",
    "xs.append(2.5)\n",
    "ys.append(17)\n",
    "\n",
    "xs.append(1.5)\n",
    "ys.append(-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/scipy/stats/stats.py:1535: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=8\n",
      "  \"anyway, n=%i\" % int(n))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>QuantReg Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>y</td>        <th>  Pseudo R-squared:  </th> <td>  0.3600</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>             <td>QuantReg</td>     <th>  Bandwidth:         </th> <td>   19.62</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>          <td>Least Squares</td>  <th>  Sparsity:          </th> <td>   28.42</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Tue, 13 Dec 2016</td> <th>  No. Observations:  </th>  <td>     8</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>16:47:36</td>     <th>  Df Residuals:      </th>  <td>     6</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  Df Model:          </th>  <td>     1</td> \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    1.0000</td> <td>    7.366</td> <td>    0.136</td> <td> 0.896</td> <td>  -17.023    19.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x</th>         <td>    3.0000</td> <td>    3.315</td> <td>    0.905</td> <td> 0.400</td> <td>   -5.111    11.111</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                         QuantReg Regression Results                          \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   Pseudo R-squared:               0.3600\n",
       "Model:                       QuantReg   Bandwidth:                       19.62\n",
       "Method:                 Least Squares   Sparsity:                        28.42\n",
       "Date:                Tue, 13 Dec 2016   No. Observations:                    8\n",
       "Time:                        16:47:36   Df Residuals:                        6\n",
       "                                        Df Model:                            1\n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      1.0000      7.366      0.136      0.896       -17.023    19.023\n",
       "x              3.0000      3.315      0.905      0.400        -5.111    11.111\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(np.array([xs, ys]).transpose(), columns=[\"x\", \"y\"])\n",
    "df.columns = [\"x\", \"y\"]\n",
    "mod = smf.quantreg('y ~ x', df)\n",
    "res = mod.fit(q=.5)\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generated a fit of $y = 2 x + 0.2420$. Let's see what a linear regression yields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/scipy/stats/stats.py:1535: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=8\n",
      "  \"anyway, n=%i\" % int(n))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.477</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.390</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   5.473</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 13 Dec 2016</td> <th>  Prob (F-statistic):</th>  <td>0.0579</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:47:36</td>     <th>  Log-Likelihood:    </th> <td> -24.966</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>     8</td>      <th>  AIC:               </th> <td>   53.93</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>     6</td>      <th>  BIC:               </th> <td>   54.09</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    0.3844</td> <td>    3.282</td> <td>    0.117</td> <td> 0.911</td> <td>   -7.647     8.416</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    3.4558</td> <td>    1.477</td> <td>    2.340</td> <td> 0.058</td> <td>   -0.159     7.070</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 2.698</td> <th>  Durbin-Watson:     </th> <td>   1.870</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.260</td> <th>  Jarque-Bera (JB):  </th> <td>   0.637</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.688</td> <th>  Prob(JB):          </th> <td>   0.727</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.141</td> <th>  Cond. No.          </th> <td>    3.64</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.477\n",
       "Model:                            OLS   Adj. R-squared:                  0.390\n",
       "Method:                 Least Squares   F-statistic:                     5.473\n",
       "Date:                Tue, 13 Dec 2016   Prob (F-statistic):             0.0579\n",
       "Time:                        16:47:36   Log-Likelihood:                -24.966\n",
       "No. Observations:                   8   AIC:                             53.93\n",
       "Df Residuals:                       6   BIC:                             54.09\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
       "------------------------------------------------------------------------------\n",
       "const          0.3844      3.282      0.117      0.911        -7.647     8.416\n",
       "x1             3.4558      1.477      2.340      0.058        -0.159     7.070\n",
       "==============================================================================\n",
       "Omnibus:                        2.698   Durbin-Watson:                   1.870\n",
       "Prob(Omnibus):                  0.260   Jarque-Bera (JB):                0.637\n",
       "Skew:                          -0.688   Prob(JB):                        0.727\n",
       "Kurtosis:                       3.141   Cond. No.                         3.64\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "X = np.array(xs).transpose()\n",
    "X = sm.add_constant(X)\n",
    "# Fit and summarize OLS model\n",
    "mod = sm.OLS(ys, X)\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This yielded a fit of $y = 1.5 x + 2.5827$.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Plot the data with both functions. Which do you think fits the data better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-eb6a07aeb7af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Least AD\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Least Squares\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   3159\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3160\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3161\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3162\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3163\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwashold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1817\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1818\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1819\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1820\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1383\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_grab_next_args\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mseg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mseg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x and y must have same first dimension\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x and y can be no greater than 2-D\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlcAAAJQCAYAAACjPkE8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAQAUElcAAIXEFQBAIXEFAFDo/wOx2SJr7gaYswAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118d28828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f1 = lambda x: 3 * x + 1\n",
    "f2 = lambda x: 3.4558 * x + 0.3844\n",
    "\n",
    "plt.scatter(xs, ys)\n",
    "plt.plot(xs, map(f1, xs), label=\"Least AD\")\n",
    "plt.plot(xs, map(f2, xs), label=\"Least Squares\")\n",
    "ax = plt.gca()\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's explore another scenario. Linear regression has five major assumptions, one of which is called _constant variance_ or _homoscedasticity_. It means that the errors are distributed with the same variance about the best fit line regardless of the value of the independent variables.\n",
    "\n",
    "For example, a persistant level of background noise can cause regression metrics to be poorly estimated. Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from scipy.stats import norm\n",
    "# Generate some data\n",
    "xs = list(np.arange(0, 10, 0.1))\n",
    "ys = [2*x + norm.pdf(0, 1) for x in xs]\n",
    "# Add random background noise\n",
    "xs2 = [10 * random.random() for i in range(20)]\n",
    "ys2 = [20 * random.random() for i in range(20)]\n",
    "\n",
    "# Plot the data sets\n",
    "plt.scatter(xs, ys, color='b')\n",
    "plt.scatter(xs2, ys2, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fit a line to the data\n",
    "lm = linear_model.LinearRegression()\n",
    "\n",
    "xs.extend(xs2)\n",
    "ys.extend(ys2)\n",
    "\n",
    "df = pd.DataFrame(np.array([xs, ys]).transpose(), columns=['x', 'y'])\n",
    "X = df[['x']]\n",
    "y = df['y']\n",
    "\n",
    "model = lm.fit(X, y)\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Plot the data and the best fit line\n",
    "## The data\n",
    "plt.scatter(X, y)\n",
    "## The line / model\n",
    "plt.plot(X, predictions)\n",
    "\n",
    "plt.show()\n",
    "print \"r^2:\", model.score(X, y)\n",
    "print \"RMSE:\", mean_squared_error(ys, predictions)\n",
    "print \"MAE:\", mean_absolute_error(ys, predictions)\n",
    "print \"Coefficients:\", model.coef_, model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now try a MAE regression with statsmodels and plot it.\n",
    "# You should see a much better fit.\n",
    "df = pd.DataFrame(np.array([xs, ys]).transpose(), columns=[\"x\", \"y\"])\n",
    "df.columns = [\"x\", \"y\"]\n",
    "mod = smf.quantreg('y ~ x', df)\n",
    "res = mod.fit(q=.5)\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the data and the best fit line\n",
    "## The data\n",
    "plt.scatter(X, y)\n",
    "## The line / model\n",
    "plt.plot(X, predictions, color='g')\n",
    "plt.plot(X, [2*x + 0.2420 for x in xs], color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
