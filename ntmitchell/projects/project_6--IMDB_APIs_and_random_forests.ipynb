{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Webscraping\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "\n",
    "# Data cleaning and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "\n",
    "# Benchmarking\n",
    "from time import time\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define webscraping functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loops through a Soup ResultSet, combines it into a string\n",
    "def string_from_soup_results(soup_ResultSet = None):\n",
    "    list_of_strings = []\n",
    "    \n",
    "    for result in soup_ResultSet:\n",
    "        list_of_strings.append(result.text.strip())\n",
    "    \n",
    "    string = \", \".join(list_of_strings)\n",
    "    \n",
    "    return string\n",
    "\n",
    "\n",
    "# Extract just the movie link URL, which contains the ID.\n",
    "def imdb_top_250_movie_ID_scaper():\n",
    "    URL = \"http://www.imdb.com/chart/top\"\n",
    "    \n",
    "    soup_for_results = BeautifulSoup(urlopen(URL).read(), 'html.parser')\n",
    "    titleColumn_results = soup_for_results.find_all('td', attrs={'class': 'titleColumn'})\n",
    "    \n",
    "    results_list = []\n",
    "\n",
    "    for movie in titleColumn_results:\n",
    "        movie_link = movie.find('a')['href']\n",
    "        \n",
    "        results_list.append(movie_link)\n",
    "        \n",
    "    results_dataframe = pd.DataFrame(data = results_list, columns = [\"Movie Link\"])\n",
    "    results_dataframe['Movie ID'] = results_dataframe['Movie Link'].str.extract(\"(tt\\d+)\", expand = True)\n",
    "    return results_dataframe\n",
    "\n",
    "\n",
    "def imdb_movie_page_scraper(movie_id = None):\n",
    "#     Scrapes IMDb to get:\n",
    "#         Title\n",
    "#         IMDb rating\n",
    "#         release year\n",
    "#         genre\n",
    "#         director\n",
    "#         writers\n",
    "#         main actors\n",
    "#         MPAA rating\n",
    "#         plot keywords\n",
    "\n",
    "    web_scraping_strainer = SoupStrainer(name = ['div', 'span', 'h1'])\n",
    "\n",
    "    URL = \"http://www.imdb.com/title/\" + movie_id\n",
    "    soup_for_results = BeautifulSoup(urlopen(URL).read(), 'html.parser', parse_only = web_scraping_strainer)\n",
    "    \n",
    "    # Movie title\n",
    "    title_results = soup_for_results.find_all('div', attrs={'class': \"title_wrapper\"})[0]\n",
    "    title = title_results.find(\"h1\", attrs={'itemprop': 'name'}).text[0:-7].strip()\n",
    "    \n",
    "    # IMDb rating\n",
    "    IMDb_rating = soup_for_results.find_all('span', attrs={'itemprop': \"ratingValue\"}).pop().text\n",
    "    \n",
    "    # Release year\n",
    "    release_year = soup_for_results.find_all('span', attrs={'id': \"titleYear\"}).pop().text.replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    \n",
    "    # Genre\n",
    "    genre_results = soup_for_results.find_all('span', attrs={'class': 'itemprop', 'itemprop': 'genre'})\n",
    "    genre_string = string_from_soup_results(genre_results)\n",
    "    \n",
    "    # Director\n",
    "    directors_list = []\n",
    "    director_results = soup_for_results.find_all('span', attrs={'itemprop': 'director', 'itemtype':\"http://schema.org/Person\"})\n",
    "    for director in director_results:\n",
    "        directors_list.append(director.find('span', attrs={'class': 'itemprop', 'itemprop': 'name'}).text)\n",
    "    director_string = \", \".join(directors_list)\n",
    "    \n",
    "    # Writers\n",
    "    writers_list = []\n",
    "    writers_results = soup_for_results.find_all('span', attrs={'itemprop': 'creator', 'itemtype': \"http://schema.org/Person\"})\n",
    "    for writer in writers_results:\n",
    "        writers_list.append(writer.find('span', attrs={'class': 'itemprop', 'itemprop': 'name'}).text)\n",
    "    writers_string = \", \".join(writers_list)\n",
    "    \n",
    "    # Starring actors\n",
    "    actors_results = soup_for_results.find_all('span', attrs={'itemprop': 'actors'})\n",
    "    actors_list = []\n",
    "    for actor in actors_results:\n",
    "        actors_list.append(actor.find('span', attrs={'class': 'itemprop', 'itemprop': 'name'}).text)\n",
    "    actors_string = \", \".join(actors_list)\n",
    "\n",
    "    # MPAA rating\n",
    "    # Not all movies have them\n",
    "    try:\n",
    "        MPAA_rating = soup_for_results.find_all('span', attrs={'itemprop': \"contentRating\"}).pop().text\n",
    "    except:\n",
    "        MPAA_rating = \"\"\n",
    "\n",
    "    # Plot keywords\n",
    "    keywords_results = soup_for_results.find_all('span', attrs={'class':'itemprop', 'itemprop': 'keywords'})\n",
    "    keywords_string = string_from_soup_results(keywords_results)\n",
    "\n",
    "    combined_results = [movie_id,\n",
    "                        title,\n",
    "                        IMDb_rating, \n",
    "                        release_year, \n",
    "                        genre_string, \n",
    "                        director_string, \n",
    "                        writers_string, \n",
    "                        actors_string, \n",
    "                        MPAA_rating, \n",
    "                        keywords_string]\n",
    "\n",
    "    results_dataframe = pd.DataFrame(combined_results,  \n",
    "                                     index = [\"Movie ID\",\n",
    "                                              \"Title\",\n",
    "                                              \"IMDb Rating\", \n",
    "                                              \"Release Year\", \n",
    "                                              \"Genre\", \"Director\", \n",
    "                                              \"Writer(s)\", \n",
    "                                              \"Actors\", \n",
    "                                              \"MPAA rating\", \n",
    "                                              \"Keywords\"]).T\n",
    "    return results_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_250_IDs = imdb_top_250_movie_ID_scaper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scraped_movies_dataframe = pd.DataFrame()\n",
    "\n",
    "# Typical runtime: 7 1/2 minutes\n",
    "RUNTIME = []\n",
    "START_TIME = time()\n",
    "\n",
    "for movie in top_250_IDs[\"Movie ID\"]:\n",
    "    scraped_movies_dataframe = pd.concat(objs = [scraped_movies_dataframe, imdb_movie_page_scraper(movie)])\n",
    "    stop_time = time()\n",
    "    RUNTIME.append(stop_time - START_TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 2.0337832565307616 movies per second.\n",
      "Scraping time (min): 8.474096902211507\n"
     ]
    }
   ],
   "source": [
    "# DEBUGGING: Evaluates runtime\n",
    "print(\"Scraped {} movies per second.\".format(max(RUNTIME) / scraped_movies_dataframe.shape[0]))\n",
    "print(\"Scraping time (min): {}\".format(max(RUNTIME) / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform initial cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movies_dataframe = scraped_movies_dataframe.copy()\n",
    "\n",
    "movies_dataframe.reset_index(inplace = True)\n",
    "movies_dataframe.drop(labels = [\"index\", \"Movie ID\"], axis = 1, inplace = True)\n",
    "\n",
    "movies_dataframe.loc[:, 'IMDb Rating'] = pd.to_numeric(movies_dataframe['IMDb Rating'], errors = 'coerce')\n",
    "movies_dataframe.loc[:, 'Release Year'] = pd.to_numeric(movies_dataframe['Release Year'], errors = 'coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb ratings range: 1.30\n"
     ]
    }
   ],
   "source": [
    "print(\"IMDb ratings range: {:4.2f}\".format(movies_dataframe['IMDb Rating'].max() - movies_dataframe['IMDb Rating'].min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 13 potential IMDb ratings values to analyze, but this is spread over a small sample of data. Consider including a more diverse selection of movies.\n",
    "http://www.imdb.com/chart/moviemeter?sort=us,asc&mode=simple&page=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setting the Directors feature to categorical data\n",
    "movies_dataframe[\"Director\"] = movies_dataframe[\"Director\"].astype('category', copy = False)\n",
    "movies_dataframe = pd.concat(objs = [movies_dataframe, pd.get_dummies(movies_dataframe[\"Director\"])], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Also see if MPAA rating contains violence or sexual\n",
    "\n",
    "# movies_dataframe[\"Violent\"] = (movies_dataframe['MPAA rating'].str.find(\"violence\") > 0).map({True: 1, False: 0})\n",
    "# movies_dataframe[\"Sexual\"] = (movies_dataframe['MPAA rating'].str.find(\"sexual\") > 0).map({True: 1, False: 0})\n",
    "\n",
    "movies_dataframe[\"Violent\"] = movies_dataframe[\"MPAA rating\"].str.contains(\"violence|violent|combat|warfare\").map({True: 1, False: 0})\n",
    "movies_dataframe[\"Sexual\"] = movies_dataframe['MPAA rating'].str.contains(\"sexual|nudity\").map({True: 1, False: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Keyword scoring based on keyword frequency\n",
    "split_keywords_dataframe = movies_dataframe[\"Keywords\"].str.split(\", \", expand = True)\n",
    "keywords_array = split_keywords_dataframe.values.reshape(1,-1)[0] \n",
    "\n",
    "most_frequent_keywords = pd.Series(keywords_array).value_counts()\n",
    "\n",
    "sum_keyword_frequency_dataframe = split_keywords_dataframe.replace(to_replace = most_frequent_keywords.to_dict()).sum(axis = 1)\n",
    "movies_dataframe[\"Keyword frequency score\"] = sum_keyword_frequency_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finding most frequent genres\n",
    "split_genres_dataframe = movies_dataframe[\"Genre\"].str.split(\", \", expand = True)\n",
    "genres_array = split_genres_dataframe.values.reshape(1,-1)[0]\n",
    "\n",
    "most_frequent_genres = pd.Series(genres_array).value_counts()\n",
    "\n",
    "# Converting genres into dummy columns\n",
    "cvec = CountVectorizer(stop_words = 'english')\n",
    "genre_count_vectorizer = cvec.fit(movies_dataframe[\"Genre\"])\n",
    "genre_dataframe = pd.DataFrame(genre_count_vectorizer.transform(movies_dataframe[\"Genre\"]).todense(),\n",
    "                       columns=genre_count_vectorizer.get_feature_names())\n",
    "\n",
    "movies_dataframe = pd.concat(objs = [movies_dataframe, genre_dataframe], axis = 1)\n",
    "movies_dataframe.drop(labels = \"fi\", axis = 1, inplace = True)\n",
    "movies_dataframe.rename(columns = {\"sci\": \"sci-fi\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis with decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "mask = [\"Title\", \"IMDb Rating\", \"Release Year\", \"Genre\", \"Director\", \"Writer(s)\", \"Actors\", \"MPAA rating\", \"Keywords\"]\n",
    "X = movies_dataframe.drop(labels = mask, axis = 1)\n",
    "X[\"Keyword frequency score\"] = preprocessing.scale(X[\"Keyword frequency score\"])\n",
    "y = movies_dataframe[\"IMDb Rating\"]\n",
    "rounded_targets = ((20 * y).round(-1) / 20).astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: 'continuous'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-8e91a6cd783b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbagging_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaggingClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mbagging_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrounded_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/sklearn/ensemble/bagging.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \"\"\"\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/sklearn/ensemble/bagging.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, max_samples, max_depth, sample_weight)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;31m# Check parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/sklearn/ensemble/bagging.py\u001b[0m in \u001b[0;36m_validate_y\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    170\u001b[0m     if y_type not in ['binary', 'multiclass', 'multiclass-multioutput',\n\u001b[1;32m    171\u001b[0m             'multilabel-indicator', 'multilabel-sequences']:\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown label type: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: 'continuous'"
     ]
    }
   ],
   "source": [
    "# estimator = LinearRegression()\n",
    "# selector = RFECV(estimator, step=1, cv=5, n_jobs = -1)\n",
    "# selector = selector.fit(X, y)\n",
    "\n",
    "bagging_classifier = BaggingClassifier(DecisionTreeClassifier())\n",
    "bagging_classifier.fit(X, rounded_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Francis Ford Coppola', 'Frank Darabont', 'Nitesh Tiwari'], dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.ix[:,selector.support_].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
